{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actions \n",
    "\n",
    "def action_0():\n",
    "    return np.random.choice([1,0], p=[0.5,0.5])\n",
    "\n",
    "def action_1():\n",
    "    return np.random.choice([1, 0], p=[0.6, 0.4])\n",
    "\n",
    "def action_2():\n",
    "    return np.random.choice([1, 0], p=[0.2, 0.8])\n",
    "\n",
    "rewards = [action_0, action_1, action_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull 0 (action_0): reward=1\n",
      "Pull 1 (action_0): reward=0\n",
      "Pull 2 (action_0): reward=0\n",
      "Pull 3 (action_0): reward=0\n",
      "Pull 4 (action_0): reward=0\n",
      "Pull 5 (action_0): reward=0\n",
      "Pull 6 (action_0): reward=0\n",
      "Pull 7 (action_0): reward=1\n",
      "Pull 8 (action_0): reward=0\n",
      "Pull 9 (action_0): reward=0\n"
     ]
    }
   ],
   "source": [
    "#do action 0 for 10 times\n",
    "for i in range(10):\n",
    "    print('Pull %d (action_0): reward=%d' % (i, rewards[0]()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0: Q(a_0)=0.50\n",
      "Action 1: Q(a_1)=0.60\n",
      "Action 2: Q(a_2)=0.20\n"
     ]
    }
   ],
   "source": [
    "# Simulate action values (Q): expected reward for each action\n",
    "pulls = 100000\n",
    "\n",
    "action_values = []\n",
    "for reward in rewards:\n",
    "    value = [reward() for _ in range(pulls)]  # execute each of the actions 'pulls' times\n",
    "    action_values.append(value)\n",
    "    \n",
    "    \n",
    "for action, value in enumerate(action_values):\n",
    "    print(\"Action %d: Q(a_%d)=%.2f\" % (action, action, np.mean(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simulate the values (V), we need to define a policy\n",
    "# (Value is the expected reward given the policy I'm following)\n",
    "\n",
    "# Define a policy:\n",
    "def policy_random():\n",
    "    '''Returns which action to perform using equal probabilities for each action'''\n",
    "    return np.random.choice([0, 1, 2], p=[1/3, 1/3, 1/3])\n",
    "\n",
    "\n",
    "def policy_better():\n",
    "    ''' A better policy than random: we choose actions 0 and 1 more often than action 2'''\n",
    "    return np.random.choice([0, 1, 2], p=[0.4, 0.5, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 43510\n",
      "Average reward: V = 0.4351\n"
     ]
    }
   ],
   "source": [
    "# Simulate Values using the random policy\n",
    "total_reward = 0\n",
    "for pull in range(pulls):\n",
    "    action = policy_random()\n",
    "    total_reward += rewards[action]()\n",
    "print(\"Total reward =\", total_reward)\n",
    "print(\"Average reward: V =\", total_reward/pulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward = 52090\n",
      "Average reward: V = 0.5209\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simulate Values using the better policy\n",
    "total_reward = 0\n",
    "for pull in range(pulls):\n",
    "    action = policy_better()\n",
    "    total_reward += rewards[action]()\n",
    "print(\"Total reward =\", total_reward)\n",
    "print(\"Average reward: V =\", total_reward/pulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V* = 0.60133\n",
      "Regret: I_t = 0.08\n"
     ]
    }
   ],
   "source": [
    "# Regret of the better policy\n",
    "V_star = max([np.mean(value) for value in action_values])\n",
    "print(\"V* =\", V_star)\n",
    "\n",
    "total_regret = 0\n",
    "for pull in range(pulls):\n",
    "    total_regret += (V_star - rewards[policy_better()]())\n",
    "print('Regret: I_t = %.2f' % (total_regret/pulls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some bandit policies to explore:\n",
    "\n",
    "def policy_greedy(action_values):\n",
    "    '''Always returns the action for which the payoff is highest'''\n",
    "    best_action = np.argmax([np.mean(value) for value in action_values])\n",
    "    return best_action\n",
    "\n",
    "\n",
    "def policy_e_greedy(action_values, epsilon=0.05):\n",
    "    '''We explore with epsilon probability, and choose the best action the rest of the time'''\n",
    "    explore = np.random.choice([1, 0], p=[epsilon, 1-epsilon])\n",
    "    if explore:\n",
    "        # Random action\n",
    "        return policy_random()\n",
    "    else:\n",
    "        # Choose best action\n",
    "        return policy_greedy(action_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementing the decaying epsilon-greedy properly requires a class definition so we can store the epsilon values\n",
    "class DecayingEGreedy:\n",
    "    \n",
    "    def __init__(self, epsilon, decay=0.99, lower_bound=0):\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.lower_bound = lower_bound\n",
    "        \n",
    "    def policy(self, action_values):\n",
    "        if self.lower_bound > 0 and self.epsilon > self.lower_bound:\n",
    "            self.epsilon *= self.decay  # update epsilon\n",
    "        explore = np.random.choice([1, 0], p=[self.epsilon, 1-self.epsilon])  # explore vs exploit decision\n",
    "        if explore:\n",
    "            # Random action\n",
    "            return policy_random()\n",
    "        else:\n",
    "            # Choose best action\n",
    "            return policy_greedy(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pulls\t\tTotal reward\t\tV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erdal\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\erdal\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t\t\t500\t\t\t0.501\n",
      "2000\t\t\t981\t\t\t0.491\n",
      "3000\t\t\t1528\t\t\t0.510\n",
      "4000\t\t\t2117\t\t\t0.529\n",
      "5000\t\t\t2708\t\t\t0.542\n",
      "6000\t\t\t3310\t\t\t0.552\n",
      "7000\t\t\t3921\t\t\t0.560\n",
      "8000\t\t\t4497\t\t\t0.562\n",
      "9000\t\t\t5096\t\t\t0.566\n",
      "10000\t\t\t5684\t\t\t0.568\n",
      "11000\t\t\t6286\t\t\t0.572\n",
      "12000\t\t\t6863\t\t\t0.572\n",
      "13000\t\t\t7470\t\t\t0.575\n",
      "14000\t\t\t8071\t\t\t0.577\n",
      "15000\t\t\t8667\t\t\t0.578\n",
      "16000\t\t\t9295\t\t\t0.581\n",
      "17000\t\t\t9862\t\t\t0.580\n",
      "18000\t\t\t10476\t\t\t0.582\n",
      "19000\t\t\t11067\t\t\t0.583\n",
      "20000\t\t\t11653\t\t\t0.583\n",
      "21000\t\t\t12241\t\t\t0.583\n",
      "22000\t\t\t12851\t\t\t0.584\n",
      "23000\t\t\t13433\t\t\t0.584\n",
      "24000\t\t\t14033\t\t\t0.585\n",
      "25000\t\t\t14620\t\t\t0.585\n",
      "26000\t\t\t15216\t\t\t0.585\n",
      "27000\t\t\t15826\t\t\t0.586\n",
      "28000\t\t\t16419\t\t\t0.586\n",
      "29000\t\t\t16989\t\t\t0.586\n",
      "30000\t\t\t17580\t\t\t0.586\n",
      "31000\t\t\t18166\t\t\t0.586\n",
      "32000\t\t\t18760\t\t\t0.586\n",
      "33000\t\t\t19358\t\t\t0.587\n",
      "34000\t\t\t19942\t\t\t0.587\n",
      "35000\t\t\t20549\t\t\t0.587\n",
      "36000\t\t\t21179\t\t\t0.588\n",
      "37000\t\t\t21749\t\t\t0.588\n",
      "38000\t\t\t22347\t\t\t0.588\n",
      "39000\t\t\t22926\t\t\t0.588\n",
      "40000\t\t\t23535\t\t\t0.588\n",
      "41000\t\t\t24120\t\t\t0.588\n",
      "42000\t\t\t24716\t\t\t0.588\n",
      "43000\t\t\t25325\t\t\t0.589\n",
      "44000\t\t\t25936\t\t\t0.589\n",
      "45000\t\t\t26513\t\t\t0.589\n",
      "46000\t\t\t27080\t\t\t0.589\n",
      "47000\t\t\t27678\t\t\t0.589\n",
      "48000\t\t\t28267\t\t\t0.589\n",
      "49000\t\t\t28866\t\t\t0.589\n",
      "50000\t\t\t29457\t\t\t0.589\n",
      "51000\t\t\t30067\t\t\t0.590\n",
      "52000\t\t\t30656\t\t\t0.590\n",
      "53000\t\t\t31224\t\t\t0.589\n",
      "54000\t\t\t31812\t\t\t0.589\n",
      "55000\t\t\t32400\t\t\t0.589\n",
      "56000\t\t\t33012\t\t\t0.590\n",
      "57000\t\t\t33628\t\t\t0.590\n",
      "58000\t\t\t34231\t\t\t0.590\n",
      "59000\t\t\t34817\t\t\t0.590\n",
      "60000\t\t\t35435\t\t\t0.591\n",
      "61000\t\t\t36032\t\t\t0.591\n",
      "62000\t\t\t36627\t\t\t0.591\n",
      "63000\t\t\t37204\t\t\t0.591\n",
      "64000\t\t\t37787\t\t\t0.590\n",
      "65000\t\t\t38375\t\t\t0.590\n",
      "66000\t\t\t38967\t\t\t0.590\n",
      "67000\t\t\t39570\t\t\t0.591\n",
      "68000\t\t\t40174\t\t\t0.591\n",
      "69000\t\t\t40762\t\t\t0.591\n",
      "70000\t\t\t41349\t\t\t0.591\n",
      "71000\t\t\t41948\t\t\t0.591\n",
      "72000\t\t\t42537\t\t\t0.591\n",
      "73000\t\t\t43125\t\t\t0.591\n",
      "74000\t\t\t43706\t\t\t0.591\n",
      "75000\t\t\t44298\t\t\t0.591\n",
      "76000\t\t\t44904\t\t\t0.591\n",
      "77000\t\t\t45485\t\t\t0.591\n",
      "78000\t\t\t46043\t\t\t0.590\n",
      "79000\t\t\t46637\t\t\t0.590\n",
      "80000\t\t\t47236\t\t\t0.590\n",
      "81000\t\t\t47828\t\t\t0.590\n",
      "82000\t\t\t48439\t\t\t0.591\n",
      "83000\t\t\t49014\t\t\t0.591\n",
      "84000\t\t\t49607\t\t\t0.591\n",
      "85000\t\t\t50212\t\t\t0.591\n",
      "86000\t\t\t50808\t\t\t0.591\n",
      "87000\t\t\t51386\t\t\t0.591\n",
      "88000\t\t\t52005\t\t\t0.591\n",
      "89000\t\t\t52611\t\t\t0.591\n",
      "90000\t\t\t53191\t\t\t0.591\n",
      "91000\t\t\t53788\t\t\t0.591\n",
      "92000\t\t\t54377\t\t\t0.591\n",
      "93000\t\t\t54999\t\t\t0.591\n",
      "94000\t\t\t55596\t\t\t0.591\n",
      "95000\t\t\t56188\t\t\t0.591\n",
      "96000\t\t\t56768\t\t\t0.591\n",
      "97000\t\t\t57372\t\t\t0.591\n",
      "98000\t\t\t57984\t\t\t0.592\n",
      "99000\t\t\t58587\t\t\t0.592\n",
      "100000\t\t\t59174\t\t\t0.592\n"
     ]
    }
   ],
   "source": [
    "# Let's test the decaying epsilon-greedy approach\n",
    "agent = DecayingEGreedy(epsilon=0.1, decay=0.99, lower_bound=0.03)\n",
    "\n",
    "# Full problem:\n",
    "action_values = [[], [], []] # initialise values\n",
    "rewards_decaying_e_greedy = []\n",
    "total_reward = 0\n",
    "print('Number of pulls\\t\\tTotal reward\\t\\tV')\n",
    "for pull in range(pulls):\n",
    "    action = agent.policy(action_values)  # choose action according to policy\n",
    "    reward = rewards[action]()  # get reward\n",
    "    action_values[action].append(reward)  # update action_values so we make better decisions down the line\n",
    "    total_reward += reward\n",
    "    if (pull+1) % 1000 == 0:\n",
    "        print('%d\\t\\t\\t%d\\t\\t\\t%.3f' % (pull+1, total_reward, total_reward/pull))\n",
    "        rewards_decaying_e_greedy.append(total_reward/pull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
